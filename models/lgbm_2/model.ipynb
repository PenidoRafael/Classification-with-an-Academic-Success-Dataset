{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "SEED = 42 # Muito importante manter a SEED igual em todos os modelos para garantir a consistência dos dados no ensemble\n",
    "FOLDS = 5 # Muito importante manter o mesmo número de FOLDS em todos os modelos para garantir a consistência dos dados no ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sintetico = pd.read_csv('../../src/train/train.csv', index_col='id')\n",
    "original = pd.read_csv('../../src/train/original.csv')\n",
    "test = pd.read_csv('../../src/test/test.csv', index_col='id')\n",
    "\n",
    "train = pd.concat([sintetico, original], ignore_index=True)\n",
    "\n",
    "initial_features = list(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, X, y, encoder, scoring=accuracy_score, k=FOLDS):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "\n",
    "    scores = []\n",
    "    out_of_fold = []\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Fold {i + 1}\")\n",
    "        \n",
    "        X_train = X.iloc[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "\n",
    "        X_val = X.iloc[val_index]\n",
    "        y_val = y.iloc[val_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        probabilidades = model.predict_proba(X_val)\n",
    "\n",
    "        # Recuperar a predição final a partir das probabilidades\n",
    "        indices_predicoes = np.argmax(probabilidades, axis=1)\n",
    "        classes_preditas = model.classes_[indices_predicoes]\n",
    "\n",
    "        score = scoring(y_val, classes_preditas)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "        true_label = pd.Series(y_val.values, name='true')\n",
    "\n",
    "        pred_label_df = pd.DataFrame(probabilidades)\n",
    "\n",
    "        oof_pred = pd.concat([pred_label_df, true_label], axis=1, ignore_index=True)\n",
    "        oof_pred.columns = [f'pred_{encoder[model.classes_[0]]}', f'pred_{encoder[model.classes_[1]]}', f'pred_{encoder[model.classes_[2]]}', 'true']\n",
    "\n",
    "        out_of_fold.append(oof_pred)\n",
    "\n",
    "    print(f\"Score: {np.mean(scores)}\")\n",
    "    \n",
    "    return scores, out_of_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_oof(oof):\n",
    "\n",
    "    os.makedirs('oof', exist_ok=True)\n",
    "\n",
    "    for i, fold in enumerate(oof):\n",
    "        fold.to_csv(f'oof/fold_{i+1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model, X_train, y_train, X_test, encoder):\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    probabilidades = model.predict_proba(X_test)\n",
    "    pred_label_df = pd.DataFrame(probabilidades)\n",
    "\n",
    "    pred_label_df.columns = [f'pred_{encoder[model.classes_[0]]}', f'pred_{encoder[model.classes_[1]]}', f'pred_{encoder[model.classes_[2]]}']\n",
    "\n",
    "    os.makedirs('test', exist_ok=True)\n",
    "\n",
    "    pred_label_df.to_csv(f'test/test_pred.csv', index=False)\n",
    "\n",
    "    return pred_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['Marital status', 'Application mode', 'Application order', 'Course',\n",
    "        'Daytime/evening attendance', 'Previous qualification', 'Nacionality',\n",
    "        \"Mother's qualification\", \"Father's qualification\",\n",
    "        \"Mother's occupation\", \"Father's occupation\", \"Displaced\",\n",
    "        'Educational special needs', 'Debtor', 'Tuition fees up to date',\n",
    "        'Gender', 'Scholarship holder', 'International', 'Curricular units 1st sem (credited)',\n",
    "        'Curricular units 1st sem (enrolled)',\n",
    "        'Curricular units 1st sem (evaluations)',\n",
    "        'Curricular units 1st sem (approved)',\n",
    "        'Curricular units 1st sem (without evaluations)',\n",
    "        'Curricular units 2nd sem (credited)',\n",
    "        'Curricular units 2nd sem (enrolled)',\n",
    "        'Curricular units 2nd sem (evaluations)',\n",
    "        'Curricular units 2nd sem (approved)',\n",
    "        'Curricular units 2nd sem (without evaluations)']\n",
    "\n",
    "for feature in cat_features:\n",
    "    for df in [train, test]:\n",
    "        df[feature] = df[feature].astype('category')\n",
    "\n",
    "num_features = df.select_dtypes(include=['int64', 'float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['Target'].replace({\"Enrolled\": 0, \"Graduate\": 1, \"Dropout\": 2})\n",
    "X = train[initial_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {0: \"Enrolled\", 1: \"Graduate\", 2: \"Dropout\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_best_params = {'n_estimators': 1189, 'num_leaves': 44, 'min_child_samples': 15, 'learning_rate': 0.02259846401043735, 'log_max_bin': 10, 'colsample_bytree': 0.5219137979097763, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.11335128586062176}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
    "    ])\n",
    "\n",
    "lgbm = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LGBMClassifier(**lgbm_best_params)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1581\n",
      "[LightGBM] [Info] Number of data points in the train set: 64753, number of used features: 329\n",
      "[LightGBM] [Info] Start training from score -1.637915\n",
      "[LightGBM] [Info] Start training from score -0.743325\n",
      "[LightGBM] [Info] Start training from score -1.108405\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "Fold 2\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1576\n",
      "[LightGBM] [Info] Number of data points in the train set: 64753, number of used features: 326\n",
      "[LightGBM] [Info] Start training from score -1.637915\n",
      "[LightGBM] [Info] Start training from score -0.743293\n",
      "[LightGBM] [Info] Start training from score -1.108452\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "Fold 3\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1582\n",
      "[LightGBM] [Info] Number of data points in the train set: 64754, number of used features: 330\n",
      "[LightGBM] [Info] Start training from score -1.637851\n",
      "[LightGBM] [Info] Start training from score -0.743308\n",
      "[LightGBM] [Info] Start training from score -1.108467\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "Fold 4\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1581\n",
      "[LightGBM] [Info] Number of data points in the train set: 64754, number of used features: 329\n",
      "[LightGBM] [Info] Start training from score -1.637931\n",
      "[LightGBM] [Info] Start training from score -0.743308\n",
      "[LightGBM] [Info] Start training from score -1.108420\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "Fold 5\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 64754, number of used features: 327\n",
      "[LightGBM] [Info] Start training from score -1.637931\n",
      "[LightGBM] [Info] Start training from score -0.743308\n",
      "[LightGBM] [Info] Start training from score -1.108420\n",
      "[LightGBM] [Warning] Unknown parameter: log_max_bin\n",
      "Score: 0.8307553428633044\n"
     ]
    }
   ],
   "source": [
    "scores,oof = cross_validation(lgbm, X, y, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in oof:\n",
    "    fold['true'] = fold['true'].replace(encoder)\n",
    "    \n",
    "save_oof(oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função objetivo para a otimização\n",
    "def objective(trial: Trial, X, y):\n",
    "    param_grid = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 2500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 1e1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0, step=0.1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0, step=0.1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 10.0, log=True),\n",
    "        'verbose_eval':False,\n",
    "        'verbose':-1,\n",
    "    }\n",
    "\n",
    "    lgbm = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LGBMClassifier(**param_grid)\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    scores,_ = cross_validation(lgbm, X, y, encoder, k=3)\n",
    "    accuracy = np.mean(scores)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=MedianPruner())\n",
    "# study.optimize(lambda trial: objective(trial, X, y), n_trials=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Melhor conjunto de hiperparâmetros\n",
    "# print(f\"Best trial: {study.best_trial.number}\")\n",
    "# print(f\"Best value: {study.best_value}\")\n",
    "# print(f\"Best params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_best_params = {'n_estimators': 1634, 'learning_rate': 0.007256713936891003, 'num_leaves': 164, 'max_depth': 64, 'min_child_samples': 21, 'min_child_weight': 0.3465367660329096, 'subsample': 0.7000000000000001, 'colsample_bytree': 0.4, 'reg_alpha': 0.08079791517562476, 'reg_lambda': 9.816836839794332}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LGBMClassifier(**lgbm_best_params)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1543\n",
      "[LightGBM] [Info] Number of data points in the train set: 64753, number of used features: 310\n",
      "[LightGBM] [Info] Start training from score -1.637915\n",
      "[LightGBM] [Info] Start training from score -0.743325\n",
      "[LightGBM] [Info] Start training from score -1.108405\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Fold 2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1544\n",
      "[LightGBM] [Info] Number of data points in the train set: 64753, number of used features: 310\n",
      "[LightGBM] [Info] Start training from score -1.637915\n",
      "[LightGBM] [Info] Start training from score -0.743293\n",
      "[LightGBM] [Info] Start training from score -1.108452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Fold 3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1544\n",
      "[LightGBM] [Info] Number of data points in the train set: 64754, number of used features: 311\n",
      "[LightGBM] [Info] Start training from score -1.637851\n",
      "[LightGBM] [Info] Start training from score -0.743308\n",
      "[LightGBM] [Info] Start training from score -1.108467\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Fold 4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1545\n",
      "[LightGBM] [Info] Number of data points in the train set: 64754, number of used features: 311\n",
      "[LightGBM] [Info] Start training from score -1.637931\n",
      "[LightGBM] [Info] Start training from score -0.743308\n",
      "[LightGBM] [Info] Start training from score -1.108420\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Fold 5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1542\n",
      "[LightGBM] [Info] Number of data points in the train set: 64754, number of used features: 311\n",
      "[LightGBM] [Info] Start training from score -1.637931\n",
      "[LightGBM] [Info] Start training from score -0.743308\n",
      "[LightGBM] [Info] Start training from score -1.108420\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Score: 0.8317683931941634\n"
     ]
    }
   ],
   "source": [
    "scores,oof = cross_validation(lgbm, X, y, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in oof:\n",
    "    fold['true'] = fold['true'].replace(encoder)\n",
    "    \n",
    "save_oof(oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1566\n",
      "[LightGBM] [Info] Number of data points in the train set: 80942, number of used features: 321\n",
      "[LightGBM] [Info] Start training from score -1.637909\n",
      "[LightGBM] [Info] Start training from score -0.743308\n",
      "[LightGBM] [Info] Start training from score -1.108433\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_Enrolled</th>\n",
       "      <th>pred_Graduate</th>\n",
       "      <th>pred_Dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002742</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>0.993423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013872</td>\n",
       "      <td>0.980136</td>\n",
       "      <td>0.005992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.259784</td>\n",
       "      <td>0.696097</td>\n",
       "      <td>0.044119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.329280</td>\n",
       "      <td>0.525888</td>\n",
       "      <td>0.144832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.664679</td>\n",
       "      <td>0.034908</td>\n",
       "      <td>0.300414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51007</th>\n",
       "      <td>0.067967</td>\n",
       "      <td>0.095318</td>\n",
       "      <td>0.836715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51008</th>\n",
       "      <td>0.014413</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.984877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51009</th>\n",
       "      <td>0.013028</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.974582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51010</th>\n",
       "      <td>0.108327</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.877081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51011</th>\n",
       "      <td>0.010349</td>\n",
       "      <td>0.012511</td>\n",
       "      <td>0.977139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51012 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred_Enrolled  pred_Graduate  pred_Dropout\n",
       "0           0.002742       0.003835      0.993423\n",
       "1           0.013872       0.980136      0.005992\n",
       "2           0.259784       0.696097      0.044119\n",
       "3           0.329280       0.525888      0.144832\n",
       "4           0.664679       0.034908      0.300414\n",
       "...              ...            ...           ...\n",
       "51007       0.067967       0.095318      0.836715\n",
       "51008       0.014413       0.000710      0.984877\n",
       "51009       0.013028       0.012390      0.974582\n",
       "51010       0.108327       0.014593      0.877081\n",
       "51011       0.010349       0.012511      0.977139\n",
       "\n",
       "[51012 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test[initial_features]\n",
    "\n",
    "predict_test(lgbm, X, y, X_test, encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penidoEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
